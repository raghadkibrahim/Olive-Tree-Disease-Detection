{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "461168c7-ab2a-436d-91f4-00864f7a7ed6",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    " *PART II: Data Analysis* \n",
    "\n",
    "<a id='resnet'></a>\n",
    "# Advanced Modeling\n",
    "\n",
    "In this phase we will be exploring the image dataset in depth words words\n",
    "In the Exploratory Data Analysis (EDA) phase of this project, a key focus is to examine the dataset in depth for any potential biases that could mistakenly be interpreted as distinguishing features by the machine learning models later on. This includes the orientation of the leaves as well as the color composition of the backround which makes up the majority of the pixels in any given image.\n",
    "\n",
    "\n",
    "**Objectives:**\n",
    "\n",
    "- Explore the dataset  \n",
    "- Analyze color composition  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4993d4-4e4b-43b6-a439-bd8ef6190d46",
   "metadata": {},
   "source": [
    "### Set up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618fed47-9bcb-4af9-a8a7-b9c65922ea7d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2689b325-a932-44ed-bdcb-a977456003d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## IMPORT LIBRARIES ##\n",
    "\n",
    "import numpy as np #numpy\n",
    "import pandas as pd # pandas\n",
    "from matplotlib import pyplot as plt # matplot library\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "846ae315-30e5-4f91-a526-4868c85a5baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## IMPORT LIBRARIES ##\n",
    "\n",
    "import os  \n",
    "from matplotlib import image as mpimg\n",
    "from random import randint\n",
    "from PIL import Image\n",
    "from skimage.io import imread, imshow\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "##########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b28c7567-8c47-43f4-9ae9-8c326aa3168b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT LIBRARIES\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras \n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9b22b7-8f17-4a22-8da1-c4ecd6619c35",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7115aaac-e26a-4d4e-9d0f-93672915ffea",
   "metadata": {},
   "source": [
    "## CNN using Keras, Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749b5fa6-a36b-4f38-81b6-3acfa2a2075e",
   "metadata": {},
   "source": [
    "> **Process outline:**\n",
    ">\n",
    ">> **STEP 1:**\n",
    ">> Load the data as a tensor\n",
    ">> \n",
    ">> **STEP 2:** Set up a CNN Sequential Model\n",
    ">> \n",
    ">> **STEP 3:** Compile Settings\n",
    ">> \n",
    ">> **STEP 4:** Train the model\n",
    ">> \n",
    ">> **STEP 5:** Evaluate the model\n",
    ">> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7343f177-be22-47ff-a606-8f3cf30b8a5a",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33a7294-fb79-4581-81cb-0a64ace3a176",
   "metadata": {},
   "source": [
    "**STEP 1: Load the data as a tensor**  \n",
    "\n",
    "The function we'll be using to load the dataset is `keras.utils.image_dataset_from_directory` and it generates a `tf.data.Dataset` object from an image files.  \n",
    "\n",
    "These loading utilites can be combined with preprocessing layers to futher transform our input dataset before training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d193209-8c21-4242-8f73-0b02657c45e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2720 files belonging to 3 classes.\n",
      "Using 2176 files for training.\n",
      "Using 544 files for validation.\n",
      "Found 680 files belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "# Set the parameters for the split\n",
    "validation_split = 0.2\n",
    "seed = 123\n",
    "\n",
    "# Load training data\n",
    "train_images = tf.keras.utils.image_dataset_from_directory(\n",
    "    'dataset/training',\n",
    "    label_mode = \"int\",\n",
    "    color_mode = \"rgb\",\n",
    "    image_size = (600, 800),\n",
    "    batch_size = 32,\n",
    "    shuffle = False,\n",
    "    seed = seed,\n",
    "    validation_split = validation_split,\n",
    "    subset = \"training\",  # Specify this is the training subset\n",
    "    interpolation = \"bilinear\",\n",
    "    follow_links = False,\n",
    "    crop_to_aspect_ratio = False,\n",
    "    pad_to_aspect_ratio = False,\n",
    "    data_format = None,\n",
    "    verbose = True\n",
    ")\n",
    "\n",
    "# Load validation data\n",
    "validation_images = tf.keras.utils.image_dataset_from_directory(\n",
    "    'dataset/training',\n",
    "    label_mode = \"int\",\n",
    "    color_mode = \"rgb\",\n",
    "    image_size = (600, 800),\n",
    "    batch_size = 32,\n",
    "    shuffle = False,\n",
    "    seed = seed,\n",
    "    validation_split = validation_split,\n",
    "    subset = \"validation\",  # Specify this is the validation subset\n",
    "    interpolation = \"bilinear\",\n",
    "    follow_links = False,\n",
    "    crop_to_aspect_ratio = False,\n",
    "    pad_to_aspect_ratio = False,\n",
    "    data_format = None,\n",
    "    verbose = False\n",
    ")\n",
    "\n",
    "# Load test data\n",
    "test_images = keras.utils.image_dataset_from_directory(\n",
    "    'dataset/testing',\n",
    "    label_mode = \"int\",\n",
    "    class_names = None,\n",
    "    color_mode = \"rgb\",\n",
    "    image_size = (600,800),\n",
    "    batch_size = 32,\n",
    "    shuffle = False,\n",
    "    seed = seed,\n",
    "    validation_split = None,\n",
    "    subset = None,\n",
    "    interpolation = \"bilinear\",\n",
    "    data_format = None,\n",
    "    verbose = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2edf9ba6-e2bc-4263-9728-50faa3e3ab47",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660723f2-88f0-4d8a-8f24-7fbb78aaf1c4",
   "metadata": {},
   "source": [
    "**STEP 2: Set up a CNN Sequential Model**\n",
    "\n",
    "A Sequential model is appropriate for a plain stack of layers where each layer has exactly one input tensor and one output tensor. A Sequential model is not appropriate when:\n",
    "\n",
    "- the model has multiple inputs or multiple outputs\n",
    "- Any of the layers has multiple inputs or multiple outputs\n",
    "\n",
    "We can create a Sequential model incrementally via the `add()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8aef1676-e837-41a5-8690-746a8dd66ce2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">598</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">798</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">299</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">399</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">297</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">397</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">148</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">198</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1875456</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │   <span style=\"color: #00af00; text-decoration-color: #00af00\">240,058,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">195</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m598\u001b[0m, \u001b[38;5;34m798\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │           \u001b[38;5;34m896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m299\u001b[0m, \u001b[38;5;34m399\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m297\u001b[0m, \u001b[38;5;34m397\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │        \u001b[38;5;34m18,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m148\u001b[0m, \u001b[38;5;34m198\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1875456\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │   \u001b[38;5;34m240,058,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │           \u001b[38;5;34m195\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">240,086,339</span> (915.86 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m240,086,339\u001b[0m (915.86 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">240,086,339</span> (915.86 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m240,086,339\u001b[0m (915.86 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a Sequential Model\n",
    "\n",
    "CNN_model = Sequential()\n",
    "\n",
    "# Create simple CNN model architecture with Pooling for dimensionality reduction \n",
    "# and Dropout to reduce overfitting\n",
    "CNN_model.add(Conv2D(32, kernel_size=(3, 3), activation = 'relu', input_shape = (600, 800, 3)))\n",
    "CNN_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "CNN_model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "CNN_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Flatten the output of our convolutional layers\n",
    "CNN_model.add(Flatten())\n",
    "\n",
    "# Add dense layers\n",
    "CNN_model.add(Dense(128, activation='relu'))\n",
    "CNN_model.add(Dense(64, activation='relu'))\n",
    "CNN_model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "# Print out a summary of the network\n",
    "CNN_model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7088a08a-017d-406d-ad58-5f0b6d0e26ea",
   "metadata": {},
   "source": [
    "**STEP 3: Compile Settings**\n",
    "\n",
    "\n",
    "In the code below, the CNN model is being compiled with the specific configurations that determine how the model will be trained. Three main components that are specified during the compilation are the following:\n",
    "\n",
    "1. **Loss Function:** The loss parameter is set to `sparse_categorical_crossentropy`. This loss function is commonly used for classification problems where the classes are mutually exclusive, meaning each data point belongs to exactly one class.\n",
    "\n",
    "2. **Optimizer:** The optimizer parameter is set to `Adam`. Adam is an optimization algorithm that adjusts the weights of the network to minimize the loss function.\n",
    "   \n",
    "4. **Metrics:** The metrics parameter includes `accuracy`. This tells the model to track the accuracy during training and validation. Accuracy is a common metric for classification tasks and provides a straightforward interpretation by expressing the proportion of inputs that were correctly classified.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3bea3aa5-df95-45d1-8d42-82a6e7cc853a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model with the desired loss function, optimizer, and metric(s) to track\n",
    "CNN_model.compile(loss = 'sparse_categorical_crossentropy',\n",
    "                  optimizer = 'Adam',\n",
    "                  metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a482fa-ca8f-4780-965a-63b3a15a0de5",
   "metadata": {},
   "source": [
    "**STEP 4: Train the Model**\n",
    "\n",
    "Now we can train the model using our training dataset. `epochs` refers to the number "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d68833d6-439a-475d-8e3b-564e779bf76c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m268s\u001b[0m 4s/step - accuracy: 0.4288 - loss: 3461.5281 - val_accuracy: 0.4320 - val_loss: 1.0446\n",
      "Epoch 2/5\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m262s\u001b[0m 4s/step - accuracy: 0.4780 - loss: 1.0015 - val_accuracy: 0.4816 - val_loss: 1.0246\n",
      "Epoch 3/5\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m259s\u001b[0m 4s/step - accuracy: 0.5942 - loss: 0.7999 - val_accuracy: 0.5846 - val_loss: 1.1771\n",
      "Epoch 4/5\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m273s\u001b[0m 4s/step - accuracy: 0.7372 - loss: 0.6113 - val_accuracy: 0.5074 - val_loss: 1.5274\n",
      "Epoch 5/5\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m299s\u001b[0m 4s/step - accuracy: 0.7845 - loss: 0.4968 - val_accuracy: 0.6342 - val_loss: 2.3302\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x140978110>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model using the training and validation datasets\n",
    "CNN_model.fit(\n",
    "    train_images,\n",
    "    epochs=5,\n",
    "    verbose=1,\n",
    "    validation_data=validation_images  # Specify the validation dataset here\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd7ea87-4eb3-43d8-8637-349b3a4ff538",
   "metadata": {},
   "source": [
    "The training accuracy has improved consistently from 42.88% to 78.45% over the 5 epochs. This indicates that the model is learning effectively from the training data. Validation Accuracy also improves but dusplays some fluctuation, starting at 43.20%, peaking at 63.42% in the last epoch, but dipping in the fourth epoch. This behavior may suggest some issues with generalization or response to model changes per epoch.\n",
    "\n",
    "On the other hand, the training loss starts unusually high at 3461.5281, which is atypical and might indicate a problem with how the loss is being calculated or an issue with the initial weights. However, it decreases to a more typical value of 0.4968 by the final epoch, showing proper learning progression.\n",
    "Validation Loss starts at 1.0446 and increases to 2.3302 by the end of the training. The increasing validation loss as training progresses is concerning and might suggest that the model is starting to overfit the training data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea144c02-8111-4e68-a160-e273ecd5d321",
   "metadata": {},
   "source": [
    "**STEP 5: Evaluate the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a3bff56-a793-48ff-a7fb-32c9a451b1f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 631ms/step - accuracy: 0.6366 - loss: 3.4305\n",
      "Test loss: 3.776289463043213\n",
      "Test accuracy: 0.6088235378265381\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model's performance on the test data\n",
    "score = CNN_model.evaluate(test_images, verbose=1)\n",
    "\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d4b362-24b8-4e71-b5c2-93857f7b3086",
   "metadata": {},
   "source": [
    "### Key Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612403d2-740b-4b89-8a31-c955f2baeb7e",
   "metadata": {},
   "source": [
    "\n",
    "The model's increasing loss on the validation set suggests it may be overfitting the training data. Overfitting happens when a model learns the details and noise in the training data to an extent that it negatively impacts the performance of the model on new data. To combat overfitting, we could increase the diversity of the training data however in this case I will be moving on to a pre-trained model, specifically ResNet-50.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25cb8b5-44a0-4339-a9c6-ba74af1f3cac",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c5d86707-6a08-4c17-8027-3a925cb5eb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"trained_CNN.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41e9f67-996e-4fe9-b2cb-6a614eac0805",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
